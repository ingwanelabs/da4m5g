{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA4 - Module 5 - Data Warehouses in the Cloud\n",
    "## Notebook 3: Loading into BigQuery\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---"
   ],
   "id": "cell-md-intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is BigQuery?\n",
    "\n",
    "**BigQuery** is Google Cloud's fully managed, serverless data warehouse.\n",
    "\n",
    "Key things to know:\n",
    "- It is designed for **analytical queries** (OLAP) on very large datasets\n",
    "- No infrastructure to manage - Google handles everything\n",
    "- It uses **standard SQL** - the same SQL you already know\n",
    "- It is organised as: **Project \u2192 Dataset \u2192 Tables** (similar to Server \u2192 Database \u2192 Tables in SSMS)\n",
    "- Today we are using project `ingwane-da4-608` and dataset `superstore_dw`\n",
    "\n",
    "| SSMS Concept | BigQuery Equivalent |\n",
    "|---|---|\n",
    "| SQL Server instance | GCP Project |\n",
    "| Database | Dataset |\n",
    "| Table | Table |\n",
    "| SSMS interface | BigQuery Console (web) |\n",
    "| SQL query | Standard SQL query |"
   ],
   "id": "cell-md-whatisbq"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Rebuild the Data Warehouse from Notebook 2\n",
    "\n"
   ],
   "id": "cell-md-step0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild everything from Notebook 2 in one block\n",
    "# We need these DataFrames ready before we can load them into BigQuery\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "GCS_URL = \"https://storage.googleapis.com/ingwane-da4/Superstore.csv\"\n",
    "staging = pd.read_csv(GCS_URL, encoding='latin1')\n",
    "staging['Order Date'] = pd.to_datetime(staging['Order Date'], dayfirst=True)\n",
    "staging['Ship Date']  = pd.to_datetime(staging['Ship Date'],  dayfirst=True)\n",
    "\n",
    "# dimCustomer\n",
    "dimCustomer = staging[['Customer ID', 'Customer Name', 'Segment']].drop_duplicates().reset_index(drop=True)\n",
    "dimCustomer.insert(0, 'Customer_SK', range(1, len(dimCustomer) + 1))\n",
    "dimCustomer.columns = ['Customer_SK', 'CustomerID', 'CustomerName', 'Segment']\n",
    "\n",
    "# dimProduct\n",
    "dimProduct = staging[['Product ID', 'Product Name', 'Category', 'Sub-Category']].drop_duplicates(subset=['Product ID']).reset_index(drop=True)\n",
    "dimProduct.insert(0, 'Product_SK', range(1, len(dimProduct) + 1))\n",
    "dimProduct.columns = ['Product_SK', 'ProductID', 'ProductName', 'Category', 'SubCategory']\n",
    "\n",
    "# dimGeography\n",
    "dimGeography = staging[['Country', 'City', 'State', 'Postal Code', 'Region']].drop_duplicates().reset_index(drop=True)\n",
    "dimGeography.insert(0, 'Geog_SK', range(1, len(dimGeography) + 1))\n",
    "dimGeography.columns = ['Geog_SK', 'Country', 'City', 'State', 'PostalCode', 'Region']\n",
    "\n",
    "# dimDate\n",
    "all_dates = pd.date_range(start=staging['Order Date'].min(), end=staging['Order Date'].max(), freq='D')\n",
    "dimDate = pd.DataFrame({'DateValue': all_dates})\n",
    "dimDate['Day']       = dimDate['DateValue'].dt.day\n",
    "dimDate['DayOfWeek'] = dimDate['DateValue'].dt.day_name()\n",
    "dimDate['Week']      = dimDate['DateValue'].dt.isocalendar().week.astype(int)\n",
    "dimDate['Month']     = dimDate['DateValue'].dt.month\n",
    "dimDate['MonthName'] = dimDate['DateValue'].dt.month_name()\n",
    "dimDate['Quarter']   = dimDate['DateValue'].dt.quarter\n",
    "dimDate['Year']      = dimDate['DateValue'].dt.year\n",
    "dimDate.insert(0, 'Date_SK', range(1, len(dimDate) + 1))\n",
    "\n",
    "# factOrderItem\n",
    "fact = staging.merge(dimCustomer[['CustomerID', 'Customer_SK']], left_on='Customer ID', right_on='CustomerID', how='left')\n",
    "fact = fact.merge(dimProduct[['ProductID', 'Product_SK']], left_on='Product ID', right_on='ProductID', how='left')\n",
    "fact = fact.merge(dimGeography[['City', 'State', 'Geog_SK']], on=['City', 'State'], how='left')\n",
    "fact = fact.merge(dimDate[['DateValue', 'Date_SK']].rename(columns={'Date_SK': 'OrderDate_SK'}), left_on='Order Date', right_on='DateValue', how='left')\n",
    "fact = fact.merge(dimDate[['DateValue', 'Date_SK']].rename(columns={'Date_SK': 'ShipDate_SK'}), left_on='Ship Date', right_on='DateValue', how='left')\n",
    "factOrderItem = fact[['Row ID', 'Order ID', 'Customer_SK', 'Product_SK', 'Geog_SK', 'OrderDate_SK', 'ShipDate_SK', 'Sales', 'Quantity', 'Discount', 'Profit']].copy()\n",
    "\n",
    "print(\"\u2705 Data warehouse rebuilt successfully\")\n",
    "print(f\"   staging       : {len(staging):,} rows\")\n",
    "print(f\"   dimCustomer   : {len(dimCustomer):,} rows\")\n",
    "print(f\"   dimProduct    : {len(dimProduct):,} rows\")\n",
    "print(f\"   dimGeography  : {len(dimGeography):,} rows\")\n",
    "print(f\"   dimDate       : {len(dimDate):,} rows\")\n",
    "print(f\"   factOrderItem : {len(factOrderItem):,} rows\")"
   ],
   "id": "cell-rebuild"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Authenticate to Google Cloud\n",
    "\n"
   ],
   "id": "cell-md-auth"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Google Cloud\n",
    "# A popup will appear - sign in with the Gmail address your trainer has added to the project\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "print(\"\u2705 Authentication complete\")"
   ],
   "id": "cell-auth"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Connect to BigQuery\n",
    "\n"
   ],
   "id": "cell-md-connect"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to BigQuery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "PROJECT_ID = 'ingwane-da4-608'\n",
    "DATASET_ID = 'superstore_dw'\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "print(f\"\u2705 Connected to BigQuery\")\n",
    "print(f\"   Project : {PROJECT_ID}\")\n",
    "print(f\"   Dataset : {DATASET_ID}\")"
   ],
   "id": "cell-connect"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the connection by listing existing datasets in the project\n",
    "\n",
    "datasets = list(client.list_datasets())\n",
    "print(\"Datasets in this project:\")\n",
    "for ds in datasets:\n",
    "    print(f\"  - {ds.dataset_id}\")"
   ],
   "id": "cell-verify"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Load Tables into BigQuery\n",
    "\n"
   ],
   "id": "cell-md-load"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load a DataFrame into BigQuery\n",
    "\n",
    "def load_to_bigquery(df, table_name):\n",
    "    destination = f\"{PROJECT_ID}.{DATASET_ID}.{table_name}\"\n",
    "    df.to_gbq(\n",
    "        destination_table=f\"{DATASET_ID}.{table_name}\",\n",
    "        project_id=PROJECT_ID,\n",
    "        if_exists='replace',\n",
    "        progress_bar=True\n",
    "    )\n",
    "    print(f\"\u2705 {table_name} loaded - {len(df):,} rows \u2192 {destination}\")\n",
    "\n",
    "print(\"Helper function ready\")"
   ],
   "id": "cell-helper"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the staging table first\n",
    "load_to_bigquery(staging, 'SuperstoreStaging')"
   ],
   "id": "cell-load-staging"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dimension tables\n",
    "load_to_bigquery(dimCustomer,  'dimCustomer')\n",
    "load_to_bigquery(dimProduct,   'dimProduct')\n",
    "load_to_bigquery(dimGeography, 'dimGeography')\n",
    "load_to_bigquery(dimDate,      'dimDate')"
   ],
   "id": "cell-load-dims"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fact table\n",
    "load_to_bigquery(factOrderItem, 'factOrderItem')"
   ],
   "id": "cell-load-fact"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Query BigQuery from Colab\n",
    "\n"
   ],
   "id": "cell-md-query"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query BigQuery using SQL - this runs against the cloud, not our pandas DataFrames\n",
    "# Note the fully qualified table name: project.dataset.table\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS total_rows,\n",
    "    ROUND(SUM(Sales), 2) AS total_sales,\n",
    "    ROUND(SUM(Profit), 2) AS total_profit\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.factOrderItem`\n",
    "\"\"\"\n",
    "\n",
    "result = client.query(sql).to_dataframe()\n",
    "print(\"Query result from BigQuery:\")\n",
    "result"
   ],
   "id": "cell-query1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more interesting query - Sales by Category using a JOIN\n",
    "# Same JOIN logic as SSMS - just different syntax for the table name\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "    p.Category,\n",
    "    COUNT(*) AS order_lines,\n",
    "    ROUND(SUM(f.Sales), 2) AS total_sales,\n",
    "    ROUND(SUM(f.Profit), 2) AS total_profit\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.factOrderItem` f\n",
    "JOIN `{PROJECT_ID}.{DATASET_ID}.dimProduct` p\n",
    "    ON f.Product_SK = p.Product_SK\n",
    "GROUP BY p.Category\n",
    "ORDER BY total_sales DESC\n",
    "\"\"\"\n",
    "\n",
    "result = client.query(sql).to_dataframe()\n",
    "print(\"Sales by Category:\")\n",
    "result"
   ],
   "id": "cell-query2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by Year - joining to dimDate\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "    d.Year,\n",
    "    COUNT(*) AS order_lines,\n",
    "    ROUND(SUM(f.Sales), 2) AS total_sales\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.factOrderItem` f\n",
    "JOIN `{PROJECT_ID}.{DATASET_ID}.dimDate` d\n",
    "    ON f.OrderDate_SK = d.Date_SK\n",
    "GROUP BY d.Year\n",
    "ORDER BY d.Year\n",
    "\"\"\"\n",
    "\n",
    "result = client.query(sql).to_dataframe()\n",
    "print(\"Sales by Year:\")\n",
    "result"
   ],
   "id": "cell-query3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcdd TASK 1 - Query Your Data Warehouse\n",
    "\n",
    "\n",
    "Write SQL queries against your BigQuery data warehouse to answer the following:\n",
    "\n",
    "1. How many customers are in **dimCustomer**?\n",
    "2. What are the **total Sales and Profit** for each **Region**? (Hint: join to dimGeography)\n",
    "3. Who are the **top 5 customers** by total Sales?\n",
    "4. **Stretch:** What is the best performing **Sub-Category** by Profit in each Region?"
   ],
   "id": "cell-md-task1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 - Question 1: How many customers are in dimCustomer?\n",
    "\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# result = client.query(sql).to_dataframe()\n",
    "# result"
   ],
   "id": "cell-task1-q1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 - Question 2: Total Sales and Profit by Region\n",
    "\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# result = client.query(sql).to_dataframe()\n",
    "# result"
   ],
   "id": "cell-task1-q2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 - Question 3: Top 5 customers by total Sales\n",
    "\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# result = client.query(sql).to_dataframe()\n",
    "# result"
   ],
   "id": "cell-task1-q3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 - Question 4 (Stretch): Best Sub-Category by Profit in each Region\n",
    "\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# result = client.query(sql).to_dataframe()\n",
    "# result"
   ],
   "id": "cell-task1-q4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2705 TASK 1 SOLUTIONS\n",
    "\n"
   ],
   "id": "cell-md-solutions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \u2615 BREAK - 15 minutes\n",
    "\n"
   ],
   "id": "cell-md-break"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Explore the BigQuery Console\n",
    "\n",
    "\n",
    "Open a new browser tab and go to:\n",
    "\n",
    "**https://console.cloud.google.com/bigquery**\n",
    "\n",
    "You should be able to see:\n",
    "- Project `ingwane-da4-608` in the left panel\n",
    "- Dataset `superstore_dw`\n",
    "- All the tables you loaded from Colab\n",
    "\n",
    "Try running a query directly in the BigQuery Console - no Python needed!"
   ],
   "id": "cell-md-console"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udd01 Day Recap\n",
    "\n",
    "\n",
    "Today you have:\n",
    "\n",
    "| What you did | Tool used |\n",
    "|---|---|\n",
    "| Loaded raw data from cloud storage | Google Colab + pandas |\n",
    "| Explored a staging table | pandas |\n",
    "| Built dimension tables | pandas |\n",
    "| Built a fact table using joins | pandas |\n",
    "| Validated your data warehouse | pandas |\n",
    "| Authenticated to Google Cloud | Google Colab auth |\n",
    "| Loaded tables into a cloud data warehouse | BigQuery |\n",
    "| Queried a real data warehouse using SQL | BigQuery |\n",
    "\n",
    "**The full ETL pipeline:**\n",
    "\n",
    "```\n",
    "Google Cloud Storage  \u2192  Google Colab (Python)  \u2192  BigQuery\n",
    "      (Extract)               (Transform)            (Load)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udc40 Thursday Preview\n",
    "\n",
    "On Thursday you will complete the official Google course:\n",
    "**Modernising Data Lakes and Data Warehouses with Google Cloud**\n",
    "\n",
    "You will go deeper into:\n",
    "- BigQuery architecture and how it works at scale\n",
    "- Loading data natively into BigQuery\n",
    "- Data Lakes vs Data Warehouses on GCP\n",
    "- Google Cloud Storage as a data lake\n",
    "\n",
    "You have already seen BigQuery in action today - Thursday will make complete sense."
   ],
   "id": "cell-md-recap"
  }
 ]
}